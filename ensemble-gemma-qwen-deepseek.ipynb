{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351c26c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T10:08:31.163260Z",
     "iopub.status.busy": "2025-09-24T10:08:31.163057Z",
     "iopub.status.idle": "2025-09-24T10:12:30.587607Z",
     "shell.execute_reply": "2025-09-24T10:12:30.586730Z"
    },
    "papermill": {
     "duration": 239.428723,
     "end_time": "2025-09-24T10:12:30.588878",
     "exception": false,
     "start_time": "2025-09-24T10:08:31.160155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 10:08:42.154760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758708522.343688      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758708522.402224      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda DTYPE: torch.float16\n",
      "MAX_LEN: 256 BATCH_SIZE: 1\n",
      "Num classes from train: 65\n",
      "\n",
      "=== Processing model: /kaggle/input/deekseepmath-7b-map-competition/MAP_EXP_09_FULL (name MAP_EXP_09_FULL) ===\n",
      "Loading model (may be memory heavy)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ed390e0a094666bf81c64778c18800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on device cuda:0 | batches: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer MAP_EXP_09_FULL: 100%|██████████| 3/3 [00:01<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote submission csv: /kaggle/working/submission_MAP_EXP_09_FULL.csv\n",
      "Wrote probabilities csv: /kaggle/working/submission_MAP_EXP_09_FULL_probabilities.csv\n",
      "Finished model: MAP_EXP_09_FULL (saved files).\n",
      "\n",
      "=== Processing model: /kaggle/input/gemma2-9b-it-cv945 (name gemma2-9b-it-cv945) ===\n",
      "Loading model (may be memory heavy)...\n",
      "[ERROR] Failed to load model /kaggle/input/gemma2-9b-it-cv945: 'NoneType' object has no attribute 'endswith'\n",
      "Skipping this model to avoid crashing the notebook.\n",
      "\n",
      "=== Processing model: /kaggle/input/qwen3-8b-map-competition/MAP_EXP_16_FULL (name MAP_EXP_16_FULL) ===\n",
      "Loading model (may be memory heavy)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e6982a7a03431991331f61ebac12f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on device cuda:0 | batches: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer MAP_EXP_16_FULL: 100%|██████████| 3/3 [00:00<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote submission csv: /kaggle/working/submission_MAP_EXP_16_FULL.csv\n",
      "Wrote probabilities csv: /kaggle/working/submission_MAP_EXP_16_FULL_probabilities.csv\n",
      "Finished model: MAP_EXP_16_FULL (saved files).\n",
      "\n",
      "=== ENSEMBLING available probability files ===\n",
      "Found probability files: ['/kaggle/working/submission_MAP_EXP_09_FULL_probabilities.csv', '/kaggle/working/submission_MAP_EXP_16_FULL_probabilities.csv']\n",
      "\n",
      "Saved ensemble submission: /kaggle/working/submission.csv\n",
      "   row_id                             Category:Misconception\n",
      "0   36696  True_Correct:NA False_Neither:NA False_Misconc...\n",
      "1   36697  False_Misconception:WNB False_Neither:NA False...\n",
      "2   36698   True_Neither:NA False_Neither:NA True_Correct:NA\n"
     ]
    }
   ],
   "source": [
    "# Full inference + ensemble pipeline (memory-safe, no bitsandbytes requirement)\n",
    "# Paste into a Kaggle notebook cell and run.\n",
    "import os, time, math, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import softmax\n",
    "\n",
    "# --------------------- CONFIG ---------------------\n",
    "# Put your model folders here (Hugging Face format saved directories in /kaggle/input/...)\n",
    "MODEL_PATHS = [\n",
    "    \"/kaggle/input/deekseepmath-7b-map-competition/MAP_EXP_09_FULL\",  # deepseek (example)\n",
    "    \"/kaggle/input/gemma2-9b-it-cv945\",                              # gemma2 lora path (example)\n",
    "    \"/kaggle/input/qwen3-8b-map-competition/MAP_EXP_16_FULL\"        # qwen3 (example)\n",
    "]\n",
    "OUT_DIR = \"/kaggle/working\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_LEN = 256           # tokens (reduce to 128 if OOM)\n",
    "BATCH_SIZE = 1          # safe default; increase if you have memory\n",
    "TOP_PRED_PER_MODEL = 25 # how many top classes and probs to save per model\n",
    "ENSEMBLE_TOPK = 3       # final top-k predictions per row\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "PRINT_EVERY = 1\n",
    "# ---------------------------------------------------\n",
    "\n",
    "print(\"DEVICE:\", DEVICE, \"DTYPE:\", DTYPE)\n",
    "print(\"MAX_LEN:\", MAX_LEN, \"BATCH_SIZE:\", BATCH_SIZE)\n",
    "\n",
    "# --------------------- DATA & LABEL ENCODER ---------------------\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "\n",
    "# fill missing and create target labels\n",
    "train['Misconception'] = train['Misconception'].fillna(\"NA\")\n",
    "train['target'] = train['Category'].astype(str) + \":\" + train['Misconception'].astype(str)\n",
    "\n",
    "le = LabelEncoder()\n",
    "train['label'] = le.fit_transform(train['target'])\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "print(\"Num classes from train:\", NUM_CLASSES)\n",
    "\n",
    "# correctness feature (powerful)\n",
    "idx_true = train['Category'].str.startswith(\"True\")\n",
    "correct_counts = (\n",
    "    train[idx_true]\n",
    "    .groupby(['QuestionId','MC_Answer'])\n",
    "    .MC_Answer.agg('count')\n",
    "    .reset_index(name='c')\n",
    "    .sort_values('c', ascending=False)\n",
    "    .drop_duplicates(['QuestionId'])\n",
    ")\n",
    "correct_counts['is_correct'] = 1\n",
    "correct_map = dict(((int(r.QuestionId), str(r.MC_Answer)), 1) for _, r in correct_counts.iterrows())\n",
    "\n",
    "# attach is_correct to test\n",
    "test = test.copy()\n",
    "test['is_correct'] = test.apply(lambda r: int(correct_map.get((int(r.QuestionId), str(r.MC_Answer)), 0)), axis=1)\n",
    "\n",
    "# prompt builder\n",
    "def build_prompt(row):\n",
    "    correctness = \"Yes\" if int(row['is_correct']) else \"No\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct? {correctness}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\"\n",
    "    )\n",
    "\n",
    "test['text'] = test.apply(build_prompt, axis=1)\n",
    "\n",
    "# --------------------- HELPERS ---------------------\n",
    "def safe_tokenizer_from(model_dir):\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
    "    except Exception as e:\n",
    "        print(\"Tokenizer load failed for\", model_dir, \":\", e)\n",
    "        raise\n",
    "    # ensure pad_token exists\n",
    "    if tok.pad_token is None:\n",
    "        if tok.eos_token is not None:\n",
    "            tok.add_special_tokens({'pad_token': tok.eos_token})\n",
    "        else:\n",
    "            tok.add_special_tokens({'pad_token': '<pad>'})\n",
    "    return tok\n",
    "\n",
    "def stream_tokenize_texts(tokenizer, texts, max_len=MAX_LEN, chunk=128):\n",
    "    \"\"\"\n",
    "    Tokenize large list of texts in smaller chunks to avoid big memory spikes.\n",
    "    Returns input_ids and attention_masks (torch tensors).\n",
    "    \"\"\"\n",
    "    input_ids_batches = []\n",
    "    attn_batches = []\n",
    "    for i in range(0, len(texts), chunk):\n",
    "        batch_texts = texts[i:i+chunk]\n",
    "        enc = tokenizer(batch_texts, truncation=True, padding='max_length', max_length=max_len, return_tensors='pt')\n",
    "        input_ids_batches.append(enc['input_ids'])\n",
    "        attn_batches.append(enc['attention_mask'])\n",
    "        # free enc\n",
    "    input_ids = torch.cat(input_ids_batches, dim=0)\n",
    "    attn = torch.cat(attn_batches, dim=0)\n",
    "    return input_ids, attn\n",
    "\n",
    "def model_name_for_path(p):\n",
    "    # create a friendly short name for outputs\n",
    "    base = os.path.basename(p.rstrip('/'))\n",
    "    if base == \"\":\n",
    "        base = p.replace('/', '_')\n",
    "    return base\n",
    "\n",
    "# --------------------- PER-MODEL INFERENCE ---------------------\n",
    "saved_prob_files = []  # will collect produced probability CSVs\n",
    "\n",
    "for model_path in MODEL_PATHS:\n",
    "    model_short = model_name_for_path(model_path)\n",
    "    print(f\"\\n=== Processing model: {model_path} (name {model_short}) ===\")\n",
    "    prob_csv = os.path.join(OUT_DIR, f\"submission_{model_short}_probabilities.csv\")\n",
    "    sub_csv  = os.path.join(OUT_DIR, f\"submission_{model_short}.csv\")\n",
    "\n",
    "    # skip if already produced\n",
    "    if os.path.exists(prob_csv) and os.path.exists(sub_csv):\n",
    "        print(\"Found existing outputs for\", model_short, \"-> skipping inference.\")\n",
    "        saved_prob_files.append(prob_csv)\n",
    "        continue\n",
    "\n",
    "    # Load tokenizer\n",
    "    try:\n",
    "        tokenizer = safe_tokenizer_from(model_path)\n",
    "    except Exception as e:\n",
    "        print(\"Skipping model due to tokenizer error:\", e)\n",
    "        continue\n",
    "\n",
    "    # Try loading model conservatively\n",
    "    try:\n",
    "        print(\"Loading model (may be memory heavy)...\")\n",
    "        cfg = AutoConfig.from_pretrained(model_path, local_files_only=True, num_labels=NUM_CLASSES)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_path,\n",
    "            config=cfg,\n",
    "            local_files_only=True,\n",
    "            torch_dtype=DTYPE,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        # ensure tie tokenizer size\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load model {model_path}: {e}\")\n",
    "        print(\"Skipping this model to avoid crashing the notebook.\")\n",
    "        # cleanup and continue\n",
    "        try:\n",
    "            del model\n",
    "        except:\n",
    "            pass\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "    # Tokenize (streamed)\n",
    "    texts = test['text'].astype(str).tolist()\n",
    "    try:\n",
    "        input_ids, attention_mask = stream_tokenize_texts(tokenizer, texts, max_len=MAX_LEN, chunk=128)\n",
    "    except Exception as e:\n",
    "        print(\"Tokenization failed:\", e)\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "    # Create DataLoader (TensorDataset)\n",
    "    ds = TensorDataset(input_ids, attention_mask)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    device_for_model = next(model.parameters()).device\n",
    "    print(\"Running inference on device\", device_for_model, \"| batches:\", len(loader))\n",
    "\n",
    "    all_logits = []\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=f\"Infer {model_short}\", disable=False):\n",
    "                ids, att = (t.to(device_for_model) for t in batch)\n",
    "                outputs = model(input_ids=ids, attention_mask=att)\n",
    "                logits = outputs.logits\n",
    "                # convert to float32 on CPU to save memory\n",
    "                all_logits.append(logits.detach().float().cpu().numpy())\n",
    "    except RuntimeError as e:\n",
    "        print(f\"[ERROR] Runtime error during inference for {model_short}: {e}\")\n",
    "        print(\"Cleaning GPU and skipping this model.\")\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "    # stack predictions and post-process\n",
    "    predictions = np.concatenate(all_logits, axis=0)  # shape (n_samples, num_classes)\n",
    "    probs = softmax(predictions, axis=1)\n",
    "\n",
    "    # top indices\n",
    "    top_indices = np.argsort(-probs, axis=1)  # each row sorted descending\n",
    "    # prepare decode: flatten and inverse transform\n",
    "    flat_idx = top_indices.flatten()\n",
    "    try:\n",
    "        decoded_flat = le.inverse_transform(flat_idx)\n",
    "    except Exception as e:\n",
    "        print(\"Label inverse transform failed:\", e)\n",
    "        # fallback: produce dummy class names (shouldn't happen)\n",
    "        decoded_flat = np.array([f\"UNK_{i}\" for i in flat_idx])\n",
    "\n",
    "    decoded_labels = decoded_flat.reshape(top_indices.shape)\n",
    "\n",
    "    # Save submission file (top-3 class strings)\n",
    "    top3 = [\" \".join(row[:3]) for row in decoded_labels]\n",
    "    sub_df = pd.DataFrame({\n",
    "        \"row_id\": test.row_id.values,\n",
    "        \"Category:Misconception\": top3\n",
    "    })\n",
    "    sub_df.to_csv(sub_csv, index=False)\n",
    "    print(\"Wrote submission csv:\", sub_csv)\n",
    "\n",
    "    # Save probabilities + top classes (top TOP_PRED_PER_MODEL)\n",
    "    rows = []\n",
    "    n = len(test)\n",
    "    for i in range(n):\n",
    "        row = {}\n",
    "        # top K names\n",
    "        topk_names = list(decoded_labels[i, :TOP_PRED_PER_MODEL])\n",
    "        row['row_id'] = int(test.row_id.values[i])\n",
    "        row['top_classes'] = \" \".join(topk_names)\n",
    "        # top probs (aligned)\n",
    "        for j in range(min(TOP_PRED_PER_MODEL, top_indices.shape[1])):\n",
    "            col = f\"prob_{j}\"\n",
    "            val = float(probs[i, top_indices[i, j]])\n",
    "            row[col] = val\n",
    "        rows.append(row)\n",
    "    prob_df = pd.DataFrame(rows)\n",
    "    prob_df.to_csv(prob_csv, index=False)\n",
    "    print(\"Wrote probabilities csv:\", prob_csv)\n",
    "\n",
    "    saved_prob_files.append(prob_csv)\n",
    "\n",
    "    # cleanup model memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"Finished model: {model_short} (saved files).\")\n",
    "\n",
    "# --------------------- ENSEMBLE ---------------------\n",
    "print(\"\\n=== ENSEMBLING available probability files ===\")\n",
    "print(\"Found probability files:\", saved_prob_files)\n",
    "if len(saved_prob_files) == 0:\n",
    "    raise FileNotFoundError(\"No model probability files found. Run inference on at least one model.\")\n",
    "\n",
    "# optionally define weights aligned to MODEL_PATHS order; if fewer available, use ones for those found\n",
    "default_weights = {model_name_for_path(p): 1.0 for p in MODEL_PATHS}\n",
    "# Example custom weights (change if desired)\n",
    "# default_weights.update({\n",
    "#     model_name_for_path(MODEL_PATHS[0]): 1.2,\n",
    "#     model_name_for_path(MODEL_PATHS[1]): 1.0,\n",
    "#     model_name_for_path(MODEL_PATHS[2]): 0.8,\n",
    "# })\n",
    "weights_list = []\n",
    "for p in saved_prob_files:\n",
    "    weights_list.append(default_weights.get(model_name_for_path(p), 1.0))\n",
    "\n",
    "# helper to load model_map row_id -> {class:prob}\n",
    "def load_prob_file(path):\n",
    "    df = pd.read_csv(path)\n",
    "    # try to infer top_classes column and prob columns\n",
    "    # top_classes may be named 'top_classes' or 'top_classes_model' etc; try to find it:\n",
    "    top_col = None\n",
    "    for c in df.columns:\n",
    "        if c.startswith(\"top_classes\"):\n",
    "            top_col = c; break\n",
    "    if top_col is None:\n",
    "        # fallback names\n",
    "        for c in ['top_classes', 'top_classes_deepseek', 'top_classes_gemma']:\n",
    "            if c in df.columns:\n",
    "                top_col = c; break\n",
    "    # collect prob columns in order\n",
    "    prob_cols = [c for c in df.columns if c.startswith(\"prob_\")]\n",
    "    # sort by numeric index\n",
    "    prob_cols = sorted(prob_cols, key=lambda x: int(''.join(filter(str.isdigit, x)) or 0))\n",
    "    model_map = {}\n",
    "    for _, row in df.iterrows():\n",
    "        rid = int(row['row_id'])\n",
    "        if top_col is None:\n",
    "            # If we can't find top names, try to reconstruct from prob columns names (not ideal)\n",
    "            # Here we just use column names as pseudo class names\n",
    "            names = [f\"cls_{i}\" for i in range(len(prob_cols))]\n",
    "        else:\n",
    "            names = str(row[top_col]).split()\n",
    "        per = {}\n",
    "        for i, pc in enumerate(prob_cols):\n",
    "            if i >= len(names): break\n",
    "            try:\n",
    "                per[names[i]] = float(row[pc])\n",
    "            except Exception:\n",
    "                per[names[i]] = 0.0\n",
    "        model_map[int(rid)] = per\n",
    "    return model_map\n",
    "\n",
    "model_maps = []\n",
    "for path in saved_prob_files:\n",
    "    try:\n",
    "        mm = load_prob_file(path)\n",
    "        model_maps.append(mm)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prob file {path}: {e}\")\n",
    "\n",
    "# ensemble per-row\n",
    "all_row_ids = sorted(set.union(*[set(mm.keys()) for mm in model_maps]))\n",
    "final_preds = {}\n",
    "for rid in all_row_ids:\n",
    "    votes = defaultdict(int)\n",
    "    total_prob = defaultdict(float)\n",
    "    max_prob = defaultdict(float)\n",
    "    for m_idx, mm in enumerate(model_maps):\n",
    "        w = weights_list[m_idx] if m_idx < len(weights_list) else 1.0\n",
    "        per = mm.get(rid, {})\n",
    "        for cls, p in per.items():\n",
    "            votes[cls] += 1\n",
    "            total_prob[cls] += p * w\n",
    "            max_prob[cls] = max(max_prob[cls], p * w)\n",
    "    # score combination\n",
    "    scores = {}\n",
    "    n_models = len(model_maps)\n",
    "    for cls in total_prob.keys():\n",
    "        scores[cls] = total_prob[cls] * 0.6 + (votes[cls] / max(1, n_models)) * 0.3 + max_prob[cls] * 0.1\n",
    "    # pick top ENSEMBLE_TOPK\n",
    "    sorted_classes = sorted(scores.items(), key=lambda x: -x[1])\n",
    "    topk = [c for c, _ in sorted_classes[:ENSEMBLE_TOPK]]\n",
    "    # pad if fewer\n",
    "    if len(topk) < ENSEMBLE_TOPK:\n",
    "        # fill with most common classes from train (safe fallback)\n",
    "        more = [c for c in le.classes_ if c not in topk]\n",
    "        topk += more[:ENSEMBLE_TOPK - len(topk)]\n",
    "    final_preds[rid] = \" \".join(topk)\n",
    "\n",
    "# build final submission aligned to test order\n",
    "out_rows = []\n",
    "for rid in test.row_id.values:\n",
    "    out_rows.append((int(rid), final_preds.get(int(rid), \" \".join(le.classes_[:ENSEMBLE_TOPK]))))\n",
    "\n",
    "final_sub = pd.DataFrame(out_rows, columns=[\"row_id\", \"Category:Misconception\"])\n",
    "final_path = os.path.join(OUT_DIR, \"submission.csv\")\n",
    "final_sub.to_csv(final_path, index=False)\n",
    "print(\"\\nSaved ensemble submission:\", final_path)\n",
    "print(final_sub.head())\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "isSourceIdPinned": false,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "datasetId": 7930680,
     "sourceId": 12559632,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7930694,
     "sourceId": 12559652,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8039184,
     "sourceId": 12719174,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8045877,
     "sourceId": 12729471,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8329305,
     "sourceId": 13146648,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 247.367688,
   "end_time": "2025-09-24T10:12:34.316202",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-24T10:08:26.948514",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1b7f2f1303ce4a10a1f60e02a20c8964": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2dfe980802a846d88106308d52c386bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2ef0e7315b6a48098ae0616c7c6c04d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_887ba230d2bc4cefaa03597b7233a605",
       "placeholder": "​",
       "style": "IPY_MODEL_f3b6591a31594168aeb91593c7852015",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "3003d88850974f1dab39cbeb8fb4c560": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "31536015202c4d638b223de598404a5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6c19902293874d0b9303f13ff898ad06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b8b5238fb2444ec89299f47dc2b81091",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3003d88850974f1dab39cbeb8fb4c560",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "6f643fb0cd83419595dd003530e3f2ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1b7f2f1303ce4a10a1f60e02a20c8964",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c3d3f8cad4994d2f92be85f38b42e099",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "78ccd898193341c2bd5380f81c749f62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8802158ce00b4a07b72c233698a78ea6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "887ba230d2bc4cefaa03597b7233a605": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8ccd214bf01c4cb3ba00e58c8b0f1fdf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ad983be3ba3f42d3abd1c7bcd09d4817": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0e6982a7a03431991331f61ebac12f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ef30547034aa4dcba3981c74686c31cf",
        "IPY_MODEL_6f643fb0cd83419595dd003530e3f2ee",
        "IPY_MODEL_e175d938352d42cea78d327c4f6b2ebd"
       ],
       "layout": "IPY_MODEL_c0f4ef55c9ce480d908968c88404608d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b0ed390e0a094666bf81c64778c18800": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2ef0e7315b6a48098ae0616c7c6c04d6",
        "IPY_MODEL_6c19902293874d0b9303f13ff898ad06",
        "IPY_MODEL_ff2256d6d33748a2ac4a09396eb56dd5"
       ],
       "layout": "IPY_MODEL_ad983be3ba3f42d3abd1c7bcd09d4817",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b71256682a234e8e8faf9fb0da045f40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8b5238fb2444ec89299f47dc2b81091": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c0f4ef55c9ce480d908968c88404608d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c3d3f8cad4994d2f92be85f38b42e099": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e175d938352d42cea78d327c4f6b2ebd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8802158ce00b4a07b72c233698a78ea6",
       "placeholder": "​",
       "style": "IPY_MODEL_2dfe980802a846d88106308d52c386bc",
       "tabbable": null,
       "tooltip": null,
       "value": " 4/4 [01:10&lt;00:00, 14.33s/it]"
      }
     },
     "ef30547034aa4dcba3981c74686c31cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_78ccd898193341c2bd5380f81c749f62",
       "placeholder": "​",
       "style": "IPY_MODEL_8ccd214bf01c4cb3ba00e58c8b0f1fdf",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "f3b6591a31594168aeb91593c7852015": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ff2256d6d33748a2ac4a09396eb56dd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b71256682a234e8e8faf9fb0da045f40",
       "placeholder": "​",
       "style": "IPY_MODEL_31536015202c4d638b223de598404a5c",
       "tabbable": null,
       "tooltip": null,
       "value": " 3/3 [02:11&lt;00:00, 37.78s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
