{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444404a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T18:08:01.558652Z",
     "iopub.status.busy": "2025-09-23T18:08:01.557926Z",
     "iopub.status.idle": "2025-09-23T18:08:01.680378Z",
     "shell.execute_reply": "2025-09-23T18:08:01.679676Z"
    },
    "papermill": {
     "duration": 0.127066,
     "end_time": "2025-09-23T18:08:01.681534",
     "exception": false,
     "start_time": "2025-09-23T18:08:01.554468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnb_wheel\r\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Find the wheel inside your dataset folder\n",
    "!ls /kaggle/input/bitsandbytes-wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "081d8c6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T18:08:01.686543Z",
     "iopub.status.busy": "2025-09-23T18:08:01.686302Z",
     "iopub.status.idle": "2025-09-23T18:08:01.805457Z",
     "shell.execute_reply": "2025-09-23T18:08:01.804799Z"
    },
    "papermill": {
     "duration": 0.122747,
     "end_time": "2025-09-23T18:08:01.806582",
     "exception": false,
     "start_time": "2025-09-23T18:08:01.683835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bitsandbytes-wheel/bnb_wheel/bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/input/bitsandbytes-wheel/bnb_wheel/bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878df38a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T18:08:01.811227Z",
     "iopub.status.busy": "2025-09-23T18:08:01.811026Z",
     "iopub.status.idle": "2025-09-23T18:09:18.057451Z",
     "shell.execute_reply": "2025-09-23T18:09:18.056679Z"
    },
    "papermill": {
     "duration": 76.250508,
     "end_time": "2025-09-23T18:09:18.058999",
     "exception": false,
     "start_time": "2025-09-23T18:08:01.808491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/bitsandbytes-wheel/bnb_wheel/bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl\r\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.47.0) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.47.0) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes==0.47.0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes==0.47.0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes==0.47.0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes==0.47.0) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes==0.47.0) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes==0.47.0) (2.4.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (4.14.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (2025.5.1)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes==0.47.0)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes==0.47.0)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes==0.47.0)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes==0.47.0)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes==0.47.0)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes==0.47.0)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes==0.47.0)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes==0.47.0)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes==0.47.0)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes==0.47.0)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes==0.47.0) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes==0.47.0) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes==0.47.0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes==0.47.0) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes==0.47.0) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes==0.47.0) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes==0.47.0) (2024.2.0)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "Successfully installed bitsandbytes-0.47.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n"
     ]
    }
   ],
   "source": [
    "!pip install  /kaggle/input/bitsandbytes-wheel/bnb_wheel/bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9f67c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T18:09:18.106581Z",
     "iopub.status.busy": "2025-09-23T18:09:18.106326Z",
     "iopub.status.idle": "2025-09-23T18:09:28.976051Z",
     "shell.execute_reply": "2025-09-23T18:09:28.975171Z"
    },
    "papermill": {
     "duration": 10.894806,
     "end_time": "2025-09-23T18:09:28.977194",
     "exception": false,
     "start_time": "2025-09-23T18:09:18.082388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitsAndBytes version: 0.47.0\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(\"BitsAndBytes version:\", bnb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580dea90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T18:09:29.026232Z",
     "iopub.status.busy": "2025-09-23T18:09:29.025924Z",
     "iopub.status.idle": "2025-09-23T18:12:08.873878Z",
     "shell.execute_reply": "2025-09-23T18:12:08.872883Z"
    },
    "papermill": {
     "duration": 159.874172,
     "end_time": "2025-09-23T18:12:08.875189",
     "exception": false,
     "start_time": "2025-09-23T18:09:29.001017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 18:09:36.096704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758650976.281881      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758650976.349401      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading datasets...\n",
      "Building prompts...\n",
      "No saved LabelEncoder found. Building from training data (must match training).\n",
      "NUM_CLASSES = 65\n",
      "\n",
      "============================================================\n",
      "Running inference for deepseek7b from /kaggle/input/deekseepmath-7b-map-competition/MAP_EXP_09_FULL\n",
      "Attempting 8-bit load for /kaggle/input/deekseepmath-7b-map-competition/MAP_EXP_09_FULL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f330d2bdd88431ba284bd12c6f8d4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 8-bit mode.\n",
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing_deepseek7b: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek7b probs shape: (3, 65)\n",
      "Saved out_inference/submission_deepseek7b.csv and out_inference/probabilities_deepseek7b.csv\n",
      "\n",
      "============================================================\n",
      "Running inference for qwen3_8b from /kaggle/input/qwen3-8b-map-competition/MAP_EXP_16_FULL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting 8-bit load for /kaggle/input/qwen3-8b-map-competition/MAP_EXP_16_FULL...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b45a07f73a34a528a07e2bb8633bdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 8-bit mode.\n",
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing_qwen3_8b: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen3_8b probs shape: (3, 65)\n",
      "Saved out_inference/submission_qwen3_8b.csv and out_inference/probabilities_qwen3_8b.csv\n",
      "\n",
      "============================================================\n",
      "Running inference for gemma2_lora from /kaggle/input/gemma2-9b-it-cv945\n",
      "Attempting 8-bit load for /kaggle/input/gemma2-9b-it-cv945...\n",
      "8-bit load failed or bitsandbytes not present: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
      "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n",
      "Trying device_map='auto' with offload_folder...\n",
      "device_map auto + offload failed: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
      "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n",
      "Falling back to CPU model load (very slow but safe).\n",
      "Failed to load model gemma2_lora Error: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
      "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n",
      "\n",
      "Ensembling model probability files...\n",
      "Saved submission_ensemble.csv\n",
      "\n",
      "Done — files in out_inference\n",
      "[PosixPath('out_inference/submission_deepseek7b.csv'), PosixPath('out_inference/probabilities_deepseek7b.csv'), PosixPath('out_inference/probabilities_qwen3_8b.csv'), PosixPath('out_inference/submission_qwen3_8b.csv')]\n"
     ]
    }
   ],
   "source": [
    "# Optimized inference notebook (inference-only) for MAP competition\n",
    "# - Runs models sequentially (not parallel) to avoid multi-GPU OOM\n",
    "# - Uses bitsandbytes 8-bit when available, otherwise device_map+offload fallback\n",
    "# - Small MAX_LEN, small batch size, streaming tokenization to reduce VRAM and speed\n",
    "# - Loads saved LabelEncoder if present, otherwise reconstructs from train\n",
    "# - Produces per-model probability CSVs and ensembled submission.csv\n",
    "\n",
    "# Usage:\n",
    "# 1) Attach Kaggle dataset that contains the trained model folders (each model_dir should contain HF model & tokenizer files and label_encoder.joblib if saved)\n",
    "# 2) Run this notebook. It will try 8-bit load first, then device_map auto + offload.\n",
    "# 3) If bitsandbytes is not installed and you want to install it, uncomment the pip install cell (may require internet which is disallowed on some competitions).\n",
    "\n",
    "# NOTE: This is inference-only. Do NOT train here.\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "# disable torch dynamo compile if present\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
    "\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Paths for model folders (these should be attached as Kaggle dataset(s))\n",
    "# Update these strings to the paths in your Kaggle notebook under /kaggle/input\n",
    "MODEL_PATHS = {\n",
    "    'deepseek7b': '/kaggle/input/deekseepmath-7b-map-competition/MAP_EXP_09_FULL',\n",
    "    'qwen3_8b':   '/kaggle/input/qwen3-8b-map-competition/MAP_EXP_16_FULL',\n",
    "    'gemma2_lora':'/kaggle/input/gemma2-9b-it-cv945'  # LoRA dir or combined model dir\n",
    "}\n",
    "\n",
    "# If you have a separate base Gemma2 checkpoint (for PEFT), point here (optional)\n",
    "GEMMA2_BASE = '/kaggle/input/gemma2-9b-it-bf16'  # or None\n",
    "\n",
    "OUT_DIR = Path('out_inference')\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MAX_LEN = 128          # smaller length to reduce compute and memory\n",
    "FORWARD_BS = 1         # per-forward batch size (GPU memory saver)\n",
    "TOKENIZE_CHUNK = 8     # how many texts to tokenize at once\n",
    "\n",
    "# Set these lower if you still OOM\n",
    "INFER_BATCH = 4        # tokenization batch (not forward) when building encodings\n",
    "\n",
    "# ----------------- Utility helpers -----------------\n",
    "\n",
    "def build_prompt(row):\n",
    "    # format must match training prompt\n",
    "    correctness = \"This answer is correct.\" if int(row.get('is_correct', 0)) else \"This answer is incorrect.\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"{correctness}\\n\"\n",
    "        f\"Student Explanation: {row.get('StudentExplanation', '')}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def safe_tokenizer_add_pad(tokenizer, model=None):\n",
    "    if tokenizer.pad_token is None:\n",
    "        pad_tok = tokenizer.eos_token or tokenizer.unk_token or '[PAD]'\n",
    "        tokenizer.add_special_tokens({'pad_token': pad_tok})\n",
    "        if model is not None:\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "def try_load_model(model_dir, num_labels, prefer_8bit=True, offload_folder='./offload'):\n",
    "    \"\"\"\n",
    "    Try load_in_8bit -> device_map auto with offload -> cpu fallback\n",
    "    Returns (model, tokenizer, device_of_model)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
    "    # try bitsandbytes 8-bit\n",
    "    model = None\n",
    "    device_of_model = 'cpu'\n",
    "    try:\n",
    "        if prefer_8bit:\n",
    "            import bitsandbytes as bnb  # noqa: F401\n",
    "            print(f\"Attempting 8-bit load for {model_dir}...\")\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_dir,\n",
    "                local_files_only=True,\n",
    "                load_in_8bit=True,\n",
    "                device_map='auto'\n",
    "            )\n",
    "            device_of_model = 'cuda'\n",
    "            print(\"Loaded in 8-bit mode.\")\n",
    "    except Exception as e:\n",
    "        print(\"8-bit load failed or bitsandbytes not present:\", e)\n",
    "\n",
    "    if model is None:\n",
    "        # try device_map auto with offload\n",
    "        try:\n",
    "            print(\"Trying device_map='auto' with offload_folder...\")\n",
    "            if os.path.exists(offload_folder):\n",
    "                shutil.rmtree(offload_folder)\n",
    "            os.makedirs(offload_folder, exist_ok=True)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_dir,\n",
    "                local_files_only=True,\n",
    "                device_map='auto',\n",
    "                offload_folder=offload_folder,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            device_of_model = 'cuda'\n",
    "            print(\"Loaded with device_map='auto' + offload.\")\n",
    "        except Exception as e:\n",
    "            print(\"device_map auto + offload failed:\", e)\n",
    "\n",
    "    if model is None:\n",
    "        # final fallback to CPU\n",
    "        print(\"Falling back to CPU model load (very slow but safe).\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_dir, local_files_only=True)\n",
    "        device_of_model = 'cpu'\n",
    "\n",
    "    safe_tokenizer_add_pad(tokenizer, model)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device_of_model\n",
    "\n",
    "\n",
    "# ----------------- Prepare data & label encoder -----------------\n",
    "print('Loading datasets...')\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "\n",
    "train['Misconception'] = train['Misconception'].fillna('NA')\n",
    "train['target'] = train['Category'].astype(str) + ':' + train['Misconception'].astype(str)\n",
    "\n",
    "# Build correctness map (same as training notebooks)\n",
    "idx_true = train['Category'].astype(str).str.startswith('True')\n",
    "correct_counts = (\n",
    "    train.loc[idx_true]\n",
    "         .groupby(['QuestionId','MC_Answer'])['MC_Answer']\n",
    "         .agg('count')\n",
    "         .reset_index(name='c')\n",
    "         .sort_values('c', ascending=False)\n",
    "         .drop_duplicates(['QuestionId'])\n",
    ")\n",
    "correct_counts['is_correct'] = 1\n",
    "# merge into test\n",
    "test = test.merge(correct_counts[['QuestionId','MC_Answer','is_correct']], on=['QuestionId','MC_Answer'], how='left')\n",
    "test['is_correct'] = test['is_correct'].fillna(0).astype(int)\n",
    "\n",
    "# Build prompt text\n",
    "print('Building prompts...')\n",
    "test['text'] = test.apply(build_prompt, axis=1)\n",
    "\n",
    "# Label encoder: try load saved encoder from first model dir, else build from train\n",
    "le = None\n",
    "saved_le_path = None\n",
    "for mp in MODEL_PATHS.values():\n",
    "    p = Path(mp) / 'label_encoder.joblib'\n",
    "    if p.exists():\n",
    "        saved_le_path = str(p)\n",
    "        break\n",
    "\n",
    "if saved_le_path:\n",
    "    print('Loading saved LabelEncoder from', saved_le_path)\n",
    "    le = joblib.load(saved_le_path)\n",
    "else:\n",
    "    print('No saved LabelEncoder found. Building from training data (must match training).')\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train['target'].values)\n",
    "\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "print('NUM_CLASSES =', NUM_CLASSES)\n",
    "\n",
    "# ----------------- Inference per model (sequential) -----------------\n",
    "# Produces per-model probs CSV with top-25 classes and per-row top-classes string\n",
    "\n",
    "PROB_FILES = []\n",
    "SUB_FILES = []\n",
    "\n",
    "for model_name, model_dir in MODEL_PATHS.items():\n",
    "    print('\\n' + '='*60)\n",
    "    print('Running inference for', model_name, 'from', model_dir)\n",
    "    try:\n",
    "        model, tokenizer, device_of_model = try_load_model(model_dir, NUM_CLASSES, prefer_8bit=True, offload_folder=f'./offload_{model_name}')\n",
    "    except Exception as e:\n",
    "        print('Failed to load model', model_name, 'Error:', e)\n",
    "        continue\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    print('Model device:', device)\n",
    "\n",
    "    # Tokenize in streaming/chunks to avoid building huge tensors\n",
    "    texts = test['text'].tolist()\n",
    "    all_probs = []\n",
    "\n",
    "    # Data collator for padding\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='pt')\n",
    "\n",
    "    # process small groups of texts to avoid OOM\n",
    "    for i in tqdm(range(0, len(texts), TOKENIZE_CHUNK), desc=f'tokenizing_{model_name}'):\n",
    "        batch_texts = texts[i:i+TOKENIZE_CHUNK]\n",
    "        enc = tokenizer(batch_texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "\n",
    "        # run forward in even smaller steps\n",
    "        for j in range(0, enc['input_ids'].size(0), FORWARD_BS):\n",
    "            b_input_ids = enc['input_ids'][j:j+FORWARD_BS].to(device)\n",
    "            b_attn = enc['attention_mask'][j:j+FORWARD_BS].to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids=b_input_ids, attention_mask=b_attn).logits\n",
    "                probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "        # free enc tensors\n",
    "        del enc\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    probs_arr = np.vstack(all_probs)\n",
    "    print(model_name, 'probs shape:', probs_arr.shape)\n",
    "\n",
    "    # compute top indices and decode\n",
    "    top_indices = np.argsort(-probs_arr, axis=1)\n",
    "    flat = top_indices.flatten()\n",
    "    decoded = le.inverse_transform(flat).reshape(top_indices.shape)\n",
    "\n",
    "    # per-model submission (top3)\n",
    "    joined_top3 = [' '.join(r[:3]) for r in decoded]\n",
    "    sub_df = pd.DataFrame({'row_id': test.row_id.values, 'Category:Misconception': joined_top3})\n",
    "    sub_file = OUT_DIR / f'submission_{model_name}.csv'\n",
    "    sub_df.to_csv(sub_file, index=False)\n",
    "    SUB_FILES.append(str(sub_file))\n",
    "\n",
    "    # save probability CSV (top-25)\n",
    "    prob_list = []\n",
    "    TOPK = min(25, probs_arr.shape[1])\n",
    "    for i in range(probs_arr.shape[0]):\n",
    "        row = {}\n",
    "        for k in range(TOPK):\n",
    "            row[f'prob_{k}'] = float(probs_arr[i, top_indices[i, k]])\n",
    "        row['row_id'] = int(test.row_id.values[i])\n",
    "        row['top_classes'] = ' '.join(decoded[i, :TOPK])\n",
    "        prob_list.append(row)\n",
    "    prob_df = pd.DataFrame(prob_list)\n",
    "    prob_file = OUT_DIR / f'probabilities_{model_name}.csv'\n",
    "    prob_df.to_csv(prob_file, index=False)\n",
    "    PROB_FILES.append(str(prob_file))\n",
    "\n",
    "    print('Saved', sub_file, 'and', prob_file)\n",
    "\n",
    "    # cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(2)\n",
    "\n",
    "# ----------------- Ensemble -----------------\n",
    "print('\\nEnsembling model probability files...')\n",
    "\n",
    "# simple weighted ensemble + agreement bonus (fast and effective)\n",
    "weights = {\n",
    "    'deepseek7b': 1.2,\n",
    "    'qwen3_8b': 1.0,\n",
    "    'gemma2_lora': 0.9\n",
    "}\n",
    "\n",
    "# load all prob files into a dict by model name\n",
    "prob_dfs = {Path(p).stem.split('_',1)[1]: pd.read_csv(p) for p in PROB_FILES}\n",
    "\n",
    "# merge them on row_id\n",
    "merged = None\n",
    "for name, df in prob_dfs.items():\n",
    "    df = df.rename(columns={c: f\"{c}_{name}\" for c in df.columns if c!='row_id'})\n",
    "    if merged is None:\n",
    "        merged = df\n",
    "    else:\n",
    "        merged = merged.merge(df, on='row_id')\n",
    "\n",
    "# build combined scores per row\n",
    "final_preds = []\n",
    "for idx, row in merged.iterrows():\n",
    "    class_scores = {}\n",
    "    # extract model-specific top classes and probs\n",
    "    for name in prob_dfs.keys():\n",
    "        top_classes = str(row[f'top_classes_{name}']).split(' ')\n",
    "        for k, cls in enumerate(top_classes):\n",
    "            prob_col = f'prob_{k}_{name}'\n",
    "            if prob_col not in row:\n",
    "                continue\n",
    "            p = float(row[prob_col])\n",
    "            w = weights.get(name, 1.0)\n",
    "            class_scores[cls] = class_scores.get(cls, 0.0) + p * w\n",
    "    # agreement bonus: count how many models predicted the class among their top K\n",
    "    # we'll use top-3 agreement bonus\n",
    "    for cls in list(class_scores.keys()):\n",
    "        votes = 0\n",
    "        for name in prob_dfs.keys():\n",
    "            top_classes = str(row[f'top_classes_{name}']).split(' ')\n",
    "            if cls in top_classes[:3]:\n",
    "                votes += 1\n",
    "        # add small bonus for agreement\n",
    "        class_scores[cls] += 0.05 * votes\n",
    "    # sort and take top-3\n",
    "    sorted_cls = sorted(class_scores.items(), key=lambda x: -x[1])\n",
    "    top3 = [c for c, s in sorted_cls[:3]]\n",
    "    final_preds.append(' '.join(top3))\n",
    "\n",
    "submission = pd.DataFrame({'row_id': merged.row_id.values, 'Category:Misconception': final_preds})\n",
    "submission.to_csv('submission_ensemble.csv', index=False)\n",
    "print('Saved submission_ensemble.csv')\n",
    "\n",
    "print('\\nDone — files in', OUT_DIR)\n",
    "print(list(OUT_DIR.iterdir()))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "datasetId": 7930680,
     "sourceId": 12559632,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7930694,
     "sourceId": 12559652,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8039184,
     "sourceId": 12719174,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8045877,
     "sourceId": 12729471,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8329305,
     "sourceId": 13146648,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 254.296195,
   "end_time": "2025-09-23T18:12:11.790898",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-23T18:07:57.494703",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f330d2bdd88431ba284bd12c6f8d4a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3125fedc9a3b420b97fcb6ed5c1c7bbb",
        "IPY_MODEL_6ae4cd5e744e4d7880c7353bbd102df7",
        "IPY_MODEL_c884280fd26940218f07722a32540c7d"
       ],
       "layout": "IPY_MODEL_c8764dda19e24f4a814fdd754aacde76",
       "tabbable": null,
       "tooltip": null
      }
     },
     "158885d803ba4ae8b26327e9aabdf1de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1b45a07f73a34a528a07e2bb8633bdf5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_88b3a917fe5b4048a9cfb49b680c2a2b",
        "IPY_MODEL_b0c261ba702546d1a7bd443d15c83f95",
        "IPY_MODEL_dc4ed2e3b71143ee81f5e3dc73a9adc1"
       ],
       "layout": "IPY_MODEL_a083722bf3104193afe9e9ed5e29ff10",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3125fedc9a3b420b97fcb6ed5c1c7bbb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7c3085eda1744456bf0a57c51d515dc2",
       "placeholder": "​",
       "style": "IPY_MODEL_37cd1f6be40d4e8aaa5210babef70309",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "37cd1f6be40d4e8aaa5210babef70309": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5a2243954bf34ceeb58be91f3723c0ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "62288e99f0cc4da08baf7cc2d42db911": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6ae4cd5e744e4d7880c7353bbd102df7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ab10451587f548bda7e26e2603743908",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_80bfc690d6f3455f883e44c91beeabea",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "6f8a4da963b249fc843e13966536c910": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f9c53aaa4d5479eb5c71b273b08dc74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "73033cf6326441209135948e46222e89": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7c3085eda1744456bf0a57c51d515dc2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e03853d86174b07b00c616ed87fae91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80bfc690d6f3455f883e44c91beeabea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "88b3a917fe5b4048a9cfb49b680c2a2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_158885d803ba4ae8b26327e9aabdf1de",
       "placeholder": "​",
       "style": "IPY_MODEL_5a2243954bf34ceeb58be91f3723c0ee",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "a083722bf3104193afe9e9ed5e29ff10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab10451587f548bda7e26e2603743908": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0c261ba702546d1a7bd443d15c83f95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6f8a4da963b249fc843e13966536c910",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b5c23c2eb0c0429cac87c5c7799821b0",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "b5c23c2eb0c0429cac87c5c7799821b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c8764dda19e24f4a814fdd754aacde76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c884280fd26940218f07722a32540c7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7e03853d86174b07b00c616ed87fae91",
       "placeholder": "​",
       "style": "IPY_MODEL_62288e99f0cc4da08baf7cc2d42db911",
       "tabbable": null,
       "tooltip": null,
       "value": " 3/3 [00:59&lt;00:00, 18.86s/it]"
      }
     },
     "dc4ed2e3b71143ee81f5e3dc73a9adc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_73033cf6326441209135948e46222e89",
       "placeholder": "​",
       "style": "IPY_MODEL_6f9c53aaa4d5479eb5c71b273b08dc74",
       "tabbable": null,
       "tooltip": null,
       "value": " 4/4 [01:08&lt;00:00, 14.00s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
